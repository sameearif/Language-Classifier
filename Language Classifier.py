# -*- coding: utf-8 -*-
"""23100088_23100091_23100126_23100107_23100067_23100062_group4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lY3pdG5cnL2-2F2FGhRC_xiHkP4EU5jl
"""

from google.colab import drive
drive.mount('/content/gdrive')

import numpy as np
import re
import glob
import pandas as pd
import math
import matplotlib.pyplot as plt

"""# DO NOT RUN"""

# We read the audio files, extracted features and saved them in csv files attached.
# The code is given below!

# import python_speech_features as mfcc
# from scipy.io.wavfile import read

# def get_MFCC(audio, sr):
# features = mfcc.mfcc(audio, sr, 0.025, 0.01, 13, appendEnergy = True)
# return np.mean(features, axis=0)

# data = []
# labels = []
# lang = {"ur": 0, "ue": 1, "en": 2}
# for filename in glob.glob("Recordings/*"):
# sr, audio = read(filename)
# features = get_MFCC(audio, sr)
# data.append(features)
# labels.append(lang[filename[11:13]])

# data = np.array(data)
# labels = np.array(labels)

# np.savetxt("Dataset.csv", data2, delimiter=",")
# np.savetxt("Labels.csv", labels, delimiter=",")

X = np.loadtxt("gdrive/MyDrive/ML/Phase2/Dataset.csv", delimiter=',')
y = np.loadtxt("gdrive/MyDrive/ML/Phase2/Labels.csv", delimiter=',')

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_predict
from sklearn.preprocessing import normalize
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import zero_one_loss
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import log_loss
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import hinge_loss
from sklearn.pipeline import make_pipeline

mean = np.mean(X, axis=0)
sd = np.std(X, axis=0)
norm_X = (X-mean)/sd

X_train, X_test, y_train, y_test = train_test_split(norm_X, y, test_size=0.2, random_state=0, shuffle=True)

"""# KNN"""

accuracyList = []
for i in range(1,11):
  knn = KNeighborsClassifier(n_neighbors=i, weights="distance", p=2)
  prediction = cross_val_predict(knn, X_train, y_train)
  matrix = np.transpose(confusion_matrix(y_train, prediction, labels=[0, 1, 2]))
  accuracy = accuracy_score(y_train, prediction)
  accuracyList.append(accuracy)
  cr = classification_report(y_train, prediction)
  print(f"k = {i}:\n")
  print("acc =", accuracy,"\n")
  print(matrix, "\n")
  print(cr,"\n\n")

plt.plot([i for i in range(1,11)], [1-a for a in accuracyList])
plt.title("VALIDATION LOSS")
plt.ylabel("loss")
plt.xlabel("k")
plt.show()

# selected k=4 due to minimum loss in graph above

knn = KNeighborsClassifier(n_neighbors=4, weights="distance", p=2)
clf = BaggingClassifier(base_estimator=knn, n_estimators=100, random_state=0, bootstrap=True, max_samples=13904, max_features=13)
clf.fit(X_train, y_train)
prediction = clf.predict(X_test)
matrix = np.transpose(confusion_matrix(y_test, prediction, labels=[0, 1, 2]))
accuracy = accuracy_score(y_test, prediction)
cr = classification_report(y_test, prediction)
print("Test results:")
print(accuracy)
print(matrix)
print(cr, )

"""# Neural Net"""

epoch = 20
LogLoss = []
for i in range(1, 6):
  clf = MLPClassifier(random_state=0, hidden_layer_sizes=(250, 250, 250, 250,), max_iter=epoch).fit(X_train, y_train)
  y_pred = cross_val_predict(clf, X_train, y_train, method="predict_proba")
  prediction = np.argmax(y_pred, axis=1)
  matrix = np.transpose(confusion_matrix(y_train, prediction, labels=[0, 1, 2]))
  accuracy = accuracy_score(y_train, prediction)
  l = log_loss(y_train, y_pred)
  LogLoss.append(l)
  cr = classification_report(y_train, prediction)
  print(f"max_iter = {epoch}:\n")
  print("acc =", accuracy,"\n")
  print(matrix, "\n")
  print(cr,"\n\n")
  epoch += 10

plt.plot([20+((i-1)*10) for i in range(1,6)], LogLoss)
plt.title("Validation LOSS")
plt.ylabel("loss")
plt.xlabel("epochs")
plt.show()

X_train, X_test, y_train, y_test = train_test_split(norm_X, y, test_size=0.2, random_state=0, shuffle=True)

clf = MLPClassifier(random_state=0, hidden_layer_sizes=(250, 250, 250, 250, ), max_iter=41).fit(X_train, y_train)
J = clf.loss_curve_

plt.plot([i for i in range(1,len(J)+1)], J)
plt.title("Training LOSS")
plt.ylabel("loss")
plt.xlabel("epochs")
plt.show()

prediction = clf.predict(X_test)
matrix = np.transpose(confusion_matrix(y_test, prediction, labels=[0, 1, 2]))
accuracy = accuracy_score(y_test, prediction)
cr = classification_report(y_test, prediction)
print("Test results:")
print(accuracy)
print(matrix)
print(cr,"\n\n")

"""# SVM"""

c = 2
hinge = []
for i in range(1, 6):
  clf = make_pipeline(StandardScaler(), SVC(random_state=0, probability=True, gamma=0.65, kernel="rbf", C=c, break_ties=True))
  y_pred = cross_val_predict(clf, X_train, y_train, method="predict_proba")
  prediction = np.argmax(y_pred, axis=1)
  matrix = np.transpose(confusion_matrix(y_train, prediction, labels=[0, 1, 2]))
  accuracy = accuracy_score(y_train, prediction)
  l = hinge_loss(y_train, y_pred)
  hinge.append(l)
  cr = classification_report(y_train, prediction)
  print(f"C = {c}:\n")
  print("accuracy =", accuracy,"\n")
  print(matrix, "\n")
  print(cr,"\n\n")
  c += 2

plt.plot([i*2 for i in range(1,6)], hinge)
plt.title("VALIDATION LOSS")
plt.ylabel("loss")
plt.xlabel("C")
plt.show()

clf = make_pipeline(StandardScaler(), SVC(random_state=0, gamma=0.65, kernel="rbf", C=1, break_ties=True, probability=True))
clf.fit(X_train, y_train)
pred = clf.predict_proba(X_train)
training_loss = hinge_loss(y_train, pred)
print("training loss = ", training_loss,"\n")

prediction = clf.predict(X_test)
matrix = np.transpose(confusion_matrix(y_test, prediction, labels=[0, 1, 2]))
accuracy = accuracy_score(y_test, prediction)
cr = classification_report(y_test, prediction)
print("Test results:")
print(accuracy)
print(matrix)
print(cr,"\n\n")

